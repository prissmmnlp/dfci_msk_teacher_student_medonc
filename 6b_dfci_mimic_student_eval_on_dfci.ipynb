{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook evaluates the DFCI-medonc-student model on the DFCI test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull in your dataset here. It should have a column labeled 'text' containing the full medonc report text \n",
    "# if you have narrative reports separate from the impressions, would concatenate the impressions at the end of the narratives.\n",
    "reports = pd.read_csv('/mnt/d/Dropbox (Partners HealthCare)/profile_3-2023/derived_data/labeled_medonc_prissmm_mixedisprog.csv')\n",
    "reports = reports[reports.split=='test']\n",
    "inference_input = reports\n",
    "inference_input['text'] = inference_input['text'].str.lower().str.replace(\"\\n\", \" \")\n",
    "inference_input.drop(inference_input.filter(regex='Unnamed|outcome').columns, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import LSTM, Linear, Embedding, Conv1d, MaxPool1d, GRU, LSTMCell, Dropout, Module, Sequential, ReLU\n",
    "\n",
    "   \n",
    "class LabeledModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LabeledModel, self).__init__()\n",
    "        \n",
    "        self.longformer = AutoModel.from_pretrained('yikuan8/Clinical-Longformer')\n",
    "        \n",
    "        self.any_cancer_head = Sequential(Linear(768, 128), ReLU(), Linear(128,1))\n",
    "        self.response_head = Sequential(Linear(768, 128), ReLU(), Linear(128,1))\n",
    "        self.progression_head = Sequential(Linear(768, 128), ReLU(), Linear(128,1))\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x_text_tensor, x_attention_mask):\n",
    "        # x should be tuple of input IDs, then attention mask\n",
    "        global_attention_mask = torch.zeros_like(x_text_tensor).to('cuda')\n",
    "        # global attention on cls token\n",
    "        global_attention_mask[:, 0] = 1\n",
    "        main = self.longformer(x_text_tensor, x_attention_mask, global_attention_mask)\n",
    "        main = main.last_hidden_state[:,0,:].squeeze(1)\n",
    "\n",
    "                                          \n",
    "        any_cancer_out = self.any_cancer_head(main)\n",
    "        response_out = self.response_head(main)\n",
    "        progression_out = self.progression_head(main)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        return any_cancer_out, response_out, progression_out\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class UnLabeledDataset(data.Dataset):\n",
    "    def __init__(self, pandas_dataset):\n",
    "        self.data = pandas_dataset.copy()\n",
    "        self.indices = self.data.index.unique()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"yikuan8/Clinical-Longformer\", truncation_side='left')        \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        # how many notes in the dataset\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # get data for notes corresponding to indices passed\n",
    "        this_index = self.indices[index]\n",
    "        pand = self.data.loc[this_index, :]\n",
    "    \n",
    "        encoded = self.tokenizer(pand['text'], padding='max_length', truncation=True)\n",
    "\n",
    "        x_text_tensor = torch.tensor(encoded.input_ids, dtype=torch.long)\n",
    "        x_attention_mask = torch.tensor(encoded.attention_mask, dtype=torch.long)\n",
    "        \n",
    "\n",
    "        return x_text_tensor, x_attention_mask\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerModel were not initialized from the model checkpoint at yikuan8/Clinical-Longformer and are newly initialized: ['longformer.pooler.dense.bias', 'longformer.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# write out inference dataset\n",
    "themodel = LabeledModel()\n",
    "themodel.load_state_dict(torch.load('dfci_mimic_note_longformer.pt'))\n",
    "themodel.to('cuda')\n",
    "\n",
    "themodel.eval()\n",
    "\n",
    "no_shuffle_valid_dataset = data.DataLoader(UnLabeledDataset(inference_input), batch_size=2, shuffle=False, num_workers=0)\n",
    "\n",
    "output_prediction_lists = [[] for x in range(3)]\n",
    "for batch in no_shuffle_valid_dataset:\n",
    "    x_text_ids = batch[0].to('cuda')\n",
    "    x_attention_mask = batch[1].to('cuda')\n",
    "    with torch.no_grad():\n",
    "        predictions = themodel(x_text_ids, x_attention_mask)\n",
    "    for j in range(3):\n",
    "        output_prediction_lists[j].append(predictions[j].detach().cpu().numpy())\n",
    "\n",
    "output_prediction_lists = [np.concatenate(x) for x in output_prediction_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataset = inference_input.copy()\n",
    "for x in range(3):\n",
    "    output_dataset['outcome_' + str(x) + '_logit'] = output_prediction_lists[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataset = output_dataset.rename(columns={'outcome_0_logit':'any_cancer_logit',\n",
    "                                                  'outcome_1_logit':'response_logit',\n",
    "                                                  'outcome_2_logit':'progression_logit'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_102023 import eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all cancers\n",
      "any_cancer\n",
      "AUC 0.9486290867049982\n",
      "Outcome probability: 0.8024837374334713\n",
      "Average precision score: 0.98\n",
      "Best F1: 0.9603636363636364\n",
      "Best F1 threshold: -0.09133487\n",
      "-0.09133487\n",
      "\n",
      "\n",
      "all cancers\n",
      "progression\n",
      "AUC 0.953051773726664\n",
      "Outcome probability: 0.17060910703725607\n",
      "Average precision score: 0.82\n",
      "Best F1: 0.7788378143972245\n",
      "Best F1 threshold: -0.0054784864\n",
      "-0.0054784864\n",
      "\n",
      "\n",
      "all cancers\n",
      "response\n",
      "AUC 0.9542077740039895\n",
      "Outcome probability: 0.11975162625665287\n",
      "Average precision score: 0.79\n",
      "Best F1: 0.7561576354679802\n",
      "Best F1 threshold: 1.1216698\n",
      "1.1216698\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for outcome in ['any_cancer','progression','response']:\n",
    "    print('all cancers')\n",
    "    print(outcome)\n",
    "    print(eval_model(output_dataset[outcome + '_logit'], output_dataset[outcome], graph=False))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataset['cancer_type'] = np.where(output_dataset.cancer_type.str.contains('nsclc'), 'nsclc', output_dataset.cancer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prostate\n",
      "any_cancer\n",
      "AUC 0.9350595507311925\n",
      "Outcome probability: 0.7528089887640449\n",
      "Average precision score: 0.98\n",
      "Best F1: 0.9272943980929679\n",
      "Best F1 threshold: -1.2145307\n",
      "-1.2145307\n",
      "\n",
      "\n",
      "prostate\n",
      "progression\n",
      "AUC 0.9348840985669631\n",
      "Outcome probability: 0.09550561797752809\n",
      "Average precision score: 0.66\n",
      "Best F1: 0.7021276595744682\n",
      "Best F1 threshold: 0.16917641\n",
      "0.16917641\n",
      "\n",
      "\n",
      "prostate\n",
      "response\n",
      "AUC 0.9442213297634985\n",
      "Outcome probability: 0.06741573033707865\n",
      "Average precision score: 0.45\n",
      "Best F1: 0.619047619047619\n",
      "Best F1 threshold: 0.6746155\n",
      "0.6746155\n",
      "\n",
      "\n",
      "breast\n",
      "any_cancer\n",
      "AUC 0.9900792372608387\n",
      "Outcome probability: 0.8935643564356436\n",
      "Average precision score: 1.00\n",
      "Best F1: 0.9862637362637363\n",
      "Best F1 threshold: -1.413629\n",
      "-1.413629\n",
      "\n",
      "\n",
      "breast\n",
      "progression\n",
      "AUC 0.9762438134930972\n",
      "Outcome probability: 0.13613861386138615\n",
      "Average precision score: 0.90\n",
      "Best F1: 0.8495575221238938\n",
      "Best F1 threshold: -1.0933954\n",
      "-1.0933954\n",
      "\n",
      "\n",
      "breast\n",
      "response\n",
      "AUC 0.9389830508474577\n",
      "Outcome probability: 0.14603960396039603\n",
      "Average precision score: 0.83\n",
      "Best F1: 0.8067226890756303\n",
      "Best F1 threshold: -0.17362471\n",
      "-0.17362471\n",
      "\n",
      "\n",
      "nsclc\n",
      "any_cancer\n",
      "AUC 0.9805130893413071\n",
      "Outcome probability: 0.8076572470373746\n",
      "Average precision score: 0.99\n",
      "Best F1: 0.9693934335002782\n",
      "Best F1 threshold: -0.09133487\n",
      "-0.09133487\n",
      "\n",
      "\n",
      "nsclc\n",
      "progression\n",
      "AUC 0.9690818963244212\n",
      "Outcome probability: 0.1959890610756609\n",
      "Average precision score: 0.90\n",
      "Best F1: 0.8497652582159626\n",
      "Best F1 threshold: 0.27400786\n",
      "0.27400786\n",
      "\n",
      "\n",
      "nsclc\n",
      "response\n",
      "AUC 0.9699678637012853\n",
      "Outcome probability: 0.15405651777575205\n",
      "Average precision score: 0.88\n",
      "Best F1: 0.8289855072463768\n",
      "Best F1 threshold: 1.1789718\n",
      "1.1789718\n",
      "\n",
      "\n",
      "pancreas\n",
      "any_cancer\n",
      "AUC 0.8112812248186946\n",
      "Outcome probability: 0.8232445520581114\n",
      "Average precision score: 0.93\n",
      "Best F1: 0.9575070821529745\n",
      "Best F1 threshold: 1.6308045\n",
      "1.6308045\n",
      "\n",
      "\n",
      "pancreas\n",
      "progression\n",
      "AUC 0.9640462189208707\n",
      "Outcome probability: 0.13075060532687652\n",
      "Average precision score: 0.77\n",
      "Best F1: 0.7884615384615384\n",
      "Best F1 threshold: 0.9872335\n",
      "0.9872335\n",
      "\n",
      "\n",
      "pancreas\n",
      "response\n",
      "AUC 0.9822974036191976\n",
      "Outcome probability: 0.09927360774818401\n",
      "Average precision score: 0.86\n",
      "Best F1: 0.8409090909090909\n",
      "Best F1 threshold: 0.82850313\n",
      "0.82850313\n",
      "\n",
      "\n",
      "rcc_barkouny\n",
      "any_cancer\n",
      "AUC 0.8064462809917355\n",
      "Outcome probability: 0.9063670411985019\n",
      "Average precision score: 0.96\n",
      "Best F1: 0.9797570850202428\n",
      "Best F1 threshold: 0.08257718\n",
      "0.08257718\n",
      "\n",
      "\n",
      "rcc_barkouny\n",
      "progression\n",
      "AUC 0.8879104477611941\n",
      "Outcome probability: 0.250936329588015\n",
      "Average precision score: 0.66\n",
      "Best F1: 0.7037037037037036\n",
      "Best F1 threshold: 3.1314263\n",
      "3.1314263\n",
      "\n",
      "\n",
      "rcc_barkouny\n",
      "response\n",
      "AUC 0.8492337578243039\n",
      "Outcome probability: 0.15355805243445692\n",
      "Average precision score: 0.58\n",
      "Best F1: 0.6352941176470588\n",
      "Best F1 threshold: 1.0662754\n",
      "1.0662754\n",
      "\n",
      "\n",
      "crc\n",
      "any_cancer\n",
      "AUC 0.9876945212222179\n",
      "Outcome probability: 0.6901408450704225\n",
      "Average precision score: 0.99\n",
      "Best F1: 0.96875\n",
      "Best F1 threshold: -1.4841521\n",
      "-1.4841521\n",
      "\n",
      "\n",
      "crc\n",
      "progression\n",
      "AUC 0.9442970822281167\n",
      "Outcome probability: 0.18309859154929578\n",
      "Average precision score: 0.82\n",
      "Best F1: 0.7741935483870969\n",
      "Best F1 threshold: 0.029641584\n",
      "0.029641584\n",
      "\n",
      "\n",
      "crc\n",
      "response\n",
      "AUC 0.9787484424754257\n",
      "Outcome probability: 0.06237424547283702\n",
      "Average precision score: 0.79\n",
      "Best F1: 0.7719298245614036\n",
      "Best F1 threshold: 1.3949374\n",
      "1.3949374\n",
      "\n",
      "\n",
      "bladder\n",
      "any_cancer\n",
      "AUC 0.9680952380952381\n",
      "Outcome probability: 0.8235294117647058\n",
      "Average precision score: 0.99\n",
      "Best F1: 0.963768115942029\n",
      "Best F1 threshold: 2.1990745\n",
      "2.1990745\n",
      "\n",
      "\n",
      "bladder\n",
      "progression\n",
      "AUC 0.8982683982683983\n",
      "Outcome probability: 0.25882352941176473\n",
      "Average precision score: 0.76\n",
      "Best F1: 0.7234042553191491\n",
      "Best F1 threshold: -0.15636003\n",
      "-0.15636003\n",
      "\n",
      "\n",
      "bladder\n",
      "response\n",
      "AUC 0.9300804828973843\n",
      "Outcome probability: 0.16470588235294117\n",
      "Average precision score: 0.75\n",
      "Best F1: 0.7796610169491526\n",
      "Best F1 threshold: 1.1755209\n",
      "1.1755209\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/Dropbox (Partners HealthCare)/impression_bert/prissmm_medonc/utils_102023.py:111: RuntimeWarning: invalid value encountered in divide\n",
      "  F1 = 2*((precision*recall)/(precision+recall))\n"
     ]
    }
   ],
   "source": [
    "for cancer in output_dataset.cancer_type.unique():\n",
    "    subset = output_dataset[output_dataset.cancer_type == cancer]\n",
    "    for outcome in ['any_cancer','progression','response']:\n",
    "        print(cancer)\n",
    "        print(outcome)\n",
    "        print(eval_model(subset[outcome + '_logit'], subset[outcome], graph=False))\n",
    "        print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
