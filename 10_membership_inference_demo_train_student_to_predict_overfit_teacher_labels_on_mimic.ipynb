{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook trains a student model on MIMIC data to predict labels assigned\n",
    "# by the DFCI teacher that was overfit to small training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes10/klkehl/miniconda3/envs/llm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic0 = pd.read_csv('../data/first_half_discharges_overfit_small_train.csv')\n",
    "mimic1 = pd.read_csv('../data/second_half_discharges_overfit_small_train.csv')\n",
    "mimic = pd.concat([mimic0,mimic1], axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 141377 entries, 0 to 70688\n",
      "Data columns (total 12 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   Unnamed: 0       141377 non-null  int64  \n",
      " 1   note_id          141377 non-null  object \n",
      " 2   subject_id       141377 non-null  int64  \n",
      " 3   hadm_id          141377 non-null  int64  \n",
      " 4   note_type        141377 non-null  object \n",
      " 5   note_seq         141377 non-null  int64  \n",
      " 6   charttime        141377 non-null  object \n",
      " 7   storetime        141375 non-null  object \n",
      " 8   text             141377 non-null  object \n",
      " 9   outcome_0_logit  141377 non-null  float64\n",
      " 10  outcome_1_logit  141377 non-null  float64\n",
      " 11  outcome_2_logit  141377 non-null  float64\n",
      "dtypes: float64(3), int64(4), object(5)\n",
      "memory usage: 14.0+ MB\n"
     ]
    }
   ],
   "source": [
    "mimic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic = mimic.rename(columns={'outcome_0_logit':'any_cancer_logit',\n",
    "                              'outcome_1_logit':'response_logit',\n",
    "                              'outcome_2_logit':'progression_logit'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_data = pd.read_csv('/data/clin_notes_outcomes/profile_3-2023/derived_data/labeled_medonc_prissmm_mixedisprog.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = phi_data[phi_data.split=='validation']\n",
    "\n",
    "validation.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import LSTM, Linear, Embedding, Conv1d, MaxPool1d, GRU, LSTMCell, Dropout, Module, Sequential, ReLU\n",
    "\n",
    "   \n",
    "class LabeledModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LabeledModel, self).__init__()\n",
    "        \n",
    "        self.longformer = AutoModel.from_pretrained('yikuan8/Clinical-Longformer')\n",
    "        \n",
    "        self.any_cancer_head = Sequential(Linear(768, 128), ReLU(), Linear(128,1))\n",
    "        self.response_head = Sequential(Linear(768, 128), ReLU(), Linear(128,1))\n",
    "        self.progression_head = Sequential(Linear(768, 128), ReLU(), Linear(128,1))\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x_text_tensor, x_attention_mask):\n",
    "        # x should be tuple of input IDs, then attention mask\n",
    "        global_attention_mask = torch.zeros_like(x_text_tensor).to('cuda:1')\n",
    "        # global attention on cls token\n",
    "        global_attention_mask[:, 0] = 1\n",
    "        main = self.longformer(x_text_tensor, x_attention_mask, global_attention_mask)\n",
    "        main = main.last_hidden_state[:,0,:].squeeze(1)\n",
    "\n",
    "                                          \n",
    "        any_cancer_out = self.any_cancer_head(main)\n",
    "        response_out = self.response_head(main)\n",
    "        progression_out = self.progression_head(main)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        return any_cancer_out, response_out, progression_out\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class PseudoLabeledDataset(data.Dataset):\n",
    "    def __init__(self, pandas_dataset):\n",
    "        self.data = pandas_dataset.copy()\n",
    "        self.indices = self.data.index.unique()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"yikuan8/Clinical-Longformer\", truncation_side='right')        \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        # how many notes in the dataset\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # get data for notes corresponding to indices passed\n",
    "        this_index = self.indices[index]\n",
    "        pand = self.data.loc[this_index, :]\n",
    "        #label = torch.tensor(pand.progression, dtype=torch.float32)\n",
    "    \n",
    "        encoded = self.tokenizer(pand['text'], padding='max_length', truncation=True)\n",
    "\n",
    "        x_text_tensor = torch.tensor(encoded.input_ids, dtype=torch.long)\n",
    "        x_attention_mask = torch.tensor(encoded.attention_mask, dtype=torch.long)\n",
    "        \n",
    "        #y_class_status = torch.tensor(pand.class_status, dtype=torch.long)\n",
    "\n",
    "        outcome_vars = [pand.any_cancer_logit, pand.response_logit, pand.progression_logit]\n",
    "        return x_text_tensor, x_attention_mask, *tuple(outcome_vars)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = PseudoLabeledDataset(mimic.reset_index(drop=True))\n",
    "\n",
    "temp_loader = data.DataLoader(temp, batch_size=4, shuffle=True)\n",
    "temp_iter = iter(temp_loader)\n",
    "a = next(temp_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([4, 4096]), torch.Size([4, 4096]), torch.Size([4]), torch.Size([4]), torch.Size([4])]\n"
     ]
    }
   ],
   "source": [
    "print([x.shape for x in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class LabeledDataset(data.Dataset):\n",
    "    def __init__(self, pandas_dataset):\n",
    "        self.data = pandas_dataset.copy()\n",
    "        self.indices = self.data.index.unique()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"yikuan8/Clinical-Longformer\", truncation_side='left')        \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        # how many notes in the dataset\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # get data for notes corresponding to indices passed\n",
    "        this_index = self.indices[index]\n",
    "        pand = self.data.loc[this_index, :]\n",
    "        #label = torch.tensor(pand.progression, dtype=torch.float32)\n",
    "    \n",
    "        encoded = self.tokenizer(pand['text'], padding='max_length', truncation=True)\n",
    "\n",
    "        x_text_tensor = torch.tensor(encoded.input_ids, dtype=torch.long)\n",
    "        x_attention_mask = torch.tensor(encoded.attention_mask, dtype=torch.long)\n",
    "        \n",
    "        y_any_cancer = torch.tensor(pand.any_cancer, dtype=torch.float32)\n",
    "        y_response = torch.tensor(pand.response, dtype=torch.float32)\n",
    "        y_progression = torch.tensor(pand.progression, dtype=torch.float32)\n",
    "        \n",
    "\n",
    "\n",
    "        return x_text_tensor, x_attention_mask, y_any_cancer, y_response, y_progression\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop\n",
    "from transformers import get_scheduler\n",
    "from torch.optim import AdamW, Adam\n",
    "#, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "def train_model(model, num_epochs, trainloader, validloader=None, device='cuda:1'):\n",
    "    \n",
    "    \n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    num_training_steps = num_epochs * len(trainloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        running_train_losses = [0.0 for i in range(3)]\n",
    "        mean_train_losses = [0.0 for i in range(3)]\n",
    "        \n",
    "        running_valid_losses = [0.0 for i in range(3)]\n",
    "        mean_valid_losses = [0.0 for i in range(3)]\n",
    "\n",
    "        num_train_batches = len(trainloader)\n",
    "                \n",
    "        model.train()\n",
    "        \n",
    "        for i, batch in enumerate(trainloader, 0):\n",
    "            input_ids = batch[0].to(device)\n",
    "            input_masks = batch[1].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs_true = [x.to(device) for x in batch[2:]]\n",
    "            \n",
    "            outputs_pred = model(input_ids, input_masks)\n",
    "            \n",
    "           \n",
    "            losses = [F.binary_cross_entropy_with_logits(outputs_pred[x].squeeze(1), torch.sigmoid(outputs_true[x])) for x in range(3)]\n",
    "            \n",
    "            total_loss = 0.0\n",
    "            for j in range(3):\n",
    "                total_loss = total_loss + losses[j]\n",
    "\n",
    "                \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "            \n",
    "            for j in range(3):\n",
    "                running_train_losses[j] += losses[j].detach().cpu().numpy()\n",
    "                mean_train_losses[j] = running_train_losses[j] / (i+1)\n",
    "                \n",
    "            print('Training Epoch: ' + str(epoch+1) + ', batch: ' + str(i + 1) + '/' + str(num_train_batches) + ' this_loss:' + str(total_loss.detach().cpu().numpy()) +', train losses: ' + str([str(x) + ': ' + str(mean_train_losses[x]) + \", \" for x in range(3)]), end='\\r', flush=True)\n",
    "        \n",
    "        print('\\n')\n",
    "        # eval on valid\n",
    "\n",
    "        torch.save(model.state_dict(), 'dfci_mimic_note_longformer_overfit_small_train.pt')\n",
    "        \n",
    "        if validloader is not None:\n",
    "            num_valid_batches = len(validloader)\n",
    "            model.eval()\n",
    "            \n",
    "            for i, batch in enumerate(validloader, 0):\n",
    "                input_ids = batch[0].to(device)\n",
    "                input_masks = batch[1].to(device)\n",
    "\n",
    "\n",
    "                outputs_true = [x.to(device) for x in batch[2:]]\n",
    "\n",
    "                outputs_pred = model(input_ids, input_masks)\n",
    "\n",
    "                losses = [F.binary_cross_entropy_with_logits(outputs_pred[x].squeeze(1), torch.sigmoid(outputs_true[x])) for x in range(3)]\n",
    "\n",
    "                total_loss = 0.0\n",
    "                for j in range(3):\n",
    "                    total_loss = total_loss + losses[j]\n",
    "                \n",
    "\n",
    "\n",
    "                for j in range(3):\n",
    "                    running_valid_losses[j] += losses[j].detach().cpu().numpy()\n",
    "\n",
    "            \n",
    "            for j in range(3):\n",
    "                mean_valid_losses[j] = running_valid_losses[j] / (i+1)\n",
    "            \n",
    "\n",
    "            \n",
    "            print('Validation Epoch: ' + str(epoch+1) + ', batch: ' + str(i + 1) + '/' + str(num_valid_batches) + ', valid losses: ' + str([str(x) + ': ' + str(mean_valid_losses[x]) + \", \" for x in range(10)]), end='\\r', flush=True)\n",
    "            print('\\n')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerModel were not initialized from the model checkpoint at yikuan8/Clinical-Longformer and are newly initialized: ['longformer.pooler.dense.bias', 'longformer.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1, batch: 70689/70689 this_loss:0.9337675783971154, train losses: ['0: 0.3815686339567896, ', '1: 0.37347575843524855, ', '2: 0.13372587011603312, ']]]\n",
      "\n",
      "Training Epoch: 2, batch: 70689/70689 this_loss:0.6156848652391111, train losses: ['0: 0.38605176772610045, ', '1: 0.40659981530350175, ', '2: 0.13526865986753986, ']\n",
      "\n",
      "Training Epoch: 3, batch: 70689/70689 this_loss:0.5826248970501404, train losses: ['0: 0.386020821962349, ', '1: 0.4065184379992952, ', '2: 0.13523275849027333, ']]]]]\n",
      "\n",
      "Training Epoch: 4, batch: 70689/70689 this_loss:0.5040451727156279, train losses: ['0: 0.38600184463521275, ', '1: 0.40643915479617976, ', '2: 0.13522169835464565, ']\n",
      "\n",
      "Training Epoch: 5, batch: 70689/70689 this_loss:0.9459959376164059, train losses: ['0: 0.3859587396688456, ', '1: 0.40643653810791114, ', '2: 0.13520631531019814, ']]\n",
      "\n",
      "Training Epoch: 6, batch: 70689/70689 this_loss:0.8748391733308205, train losses: ['0: 0.38596153224769325, ', '1: 0.40636751726761383, ', '2: 0.1351891766690695, ']]\n",
      "\n",
      "Training Epoch: 7, batch: 70689/70689 this_loss:0.6353634144555427, train losses: ['0: 0.38594890876966764, ', '1: 0.40632749284956876, ', '2: 0.13519192908424782, ']\n",
      "\n",
      "Training Epoch: 8, batch: 49592/70689 this_loss:0.7441060844777398, train losses: ['0: 0.38631582545013693, ', '1: 0.4068652514706819, ', '2: 0.13520144676616716, ']]]\r"
     ]
    }
   ],
   "source": [
    "\n",
    "themodel = LabeledModel().to('cuda:1')\n",
    "trainloader = data.DataLoader(PseudoLabeledDataset(mimic.reset_index(drop=True)), batch_size=2, num_workers=8, shuffle=True)\n",
    "#validloader = data.DataLoader(LabeledDataset(validation.reset_index(drop=True)), batch_size=4, num_workers=8, shuffle=True)\n",
    "train_model(themodel,8, trainloader, device='cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(themodel.state_dict(), 'dfci_mimic_note_longformer_overfit_small_train.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out actual validation dataset (not relevant for overfit teacher labels)\n",
    "themodel = LabeledModel()\n",
    "themodel.load_state_dict(torch.load('dfci_mimic_note_longformer_overfit_small_train.pt'))\n",
    "themodel.to('cuda')\n",
    "\n",
    "themodel.eval()\n",
    "\n",
    "no_shuffle_valid_dataset = data.DataLoader(LabeledDataset(validation), batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "output_true_lists = [[] for x in range(3)]\n",
    "output_prediction_lists = [[] for x in range(3)]\n",
    "for batch in no_shuffle_valid_dataset:\n",
    "    #thisframe = pd.DataFrame()\n",
    "    x_text_ids = batch[0].to('cuda')\n",
    "    x_attention_mask = batch[1].to('cuda')\n",
    "    with torch.no_grad():\n",
    "        predictions = themodel(x_text_ids, x_attention_mask)\n",
    "    for j in range(3):\n",
    "        output_true_lists[j].append(batch[2+j].detach().cpu().numpy())\n",
    "        output_prediction_lists[j].append(predictions[j].detach().cpu().numpy())\n",
    "\n",
    "output_true_lists = [np.concatenate(x) for x in output_true_lists]        \n",
    "output_prediction_lists = [np.concatenate(x) for x in output_prediction_lists]\n",
    "\n",
    "\n",
    "output_validation = validation.copy()\n",
    "for x in range(3):\n",
    "    output_validation['outcome_' + str(x) + '_logit'] = output_prediction_lists[x]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
