{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes10/klkehl/miniconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook is for running inference on external data with a previously trained student model\n",
    "external_data = pd.read_csv('your_data_here.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import LSTM, Linear, Embedding, Conv1d, MaxPool1d, GRU, LSTMCell, Dropout, Module, Sequential, ReLU\n",
    "\n",
    "   \n",
    "class LabeledModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LabeledModel, self).__init__()\n",
    "        \n",
    "        self.longformer = AutoModel.from_pretrained('yikuan8/Clinical-Longformer')\n",
    "        \n",
    "        self.any_cancer_head = Sequential(Linear(768, 128), ReLU(), Linear(128,1))\n",
    "        self.response_head = Sequential(Linear(768, 128), ReLU(), Linear(128,1))\n",
    "        self.progression_head = Sequential(Linear(768, 128), ReLU(), Linear(128,1))\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x_text_tensor, x_attention_mask):\n",
    "        # x should be tuple of input IDs, then attention mask\n",
    "        global_attention_mask = torch.zeros_like(x_text_tensor).to('cuda')\n",
    "        # global attention on cls token\n",
    "        global_attention_mask[:, 0] = 1\n",
    "        main = self.longformer(x_text_tensor, x_attention_mask, global_attention_mask)\n",
    "        main = main.last_hidden_state[:,0,:].squeeze(1)\n",
    "\n",
    "                                          \n",
    "        any_cancer_out = self.any_cancer_head(main)\n",
    "        response_out = self.response_head(main)\n",
    "        progression_out = self.progression_head(main)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        return any_cancer_out, response_out, progression_out\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class UnLabeledDataset(data.Dataset):\n",
    "    def __init__(self, pandas_dataset):\n",
    "        self.data = pandas_dataset.copy()\n",
    "        self.indices = self.data.index.unique()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"yikuan8/Clinical-Longformer\", truncation_side='left')        \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        # how many notes in the dataset\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # get data for notes corresponding to indices passed\n",
    "        this_index = self.indices[index]\n",
    "        pand = self.data.loc[this_index, :]\n",
    "    \n",
    "        encoded = self.tokenizer(pand['text'], padding='max_length', truncation=True)\n",
    "\n",
    "        x_text_tensor = torch.tensor(encoded.input_ids, dtype=torch.long)\n",
    "        x_attention_mask = torch.tensor(encoded.attention_mask, dtype=torch.long)\n",
    "        \n",
    "\n",
    "        return x_text_tensor, x_attention_mask\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at yikuan8/Clinical-Longformer were not used when initializing LongformerModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerModel were not initialized from the model checkpoint at yikuan8/Clinical-Longformer and are newly initialized: ['longformer.pooler.dense.bias', 'longformer.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# write out inference dataset\n",
    "themodel = LabeledModel()\n",
    "themodel.load_state_dict(torch.load('dfci_mimic_note_longformer.pt'))\n",
    "themodel.to('cuda')\n",
    "\n",
    "themodel.eval()\n",
    "\n",
    "no_shuffle_valid_dataset = data.DataLoader(UnLabeledDataset(external_data), batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "output_prediction_lists = [[] for x in range(3)]\n",
    "for batch in no_shuffle_valid_dataset:\n",
    "    x_text_ids = batch[0].to('cuda')\n",
    "    x_attention_mask = batch[1].to('cuda')\n",
    "    with torch.no_grad():\n",
    "        predictions = themodel(x_text_ids, x_attention_mask)\n",
    "    for j in range(3):\n",
    "        output_prediction_lists[j].append(predictions[j].detach().cpu().numpy())\n",
    "\n",
    "output_prediction_lists = [np.concatenate(x) for x in output_prediction_lists]\n",
    "\n",
    "\n",
    "output_external = external_data.copy()\n",
    "for x in range(3):\n",
    "    output_external['outcome_' + str(x) + '_logit'] = output_prediction_lists[x]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_external = output_external.rename(columns={'outcome_0_logit':'any_cancer_logit',\n",
    "                                                  'outcome_1_logit':'response_logit',\n",
    "                                                  'outcome_2_logit':'progression_logit'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_external.to_csv('your_output_file_here.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
