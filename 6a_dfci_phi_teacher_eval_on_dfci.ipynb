{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook runs inference for evaluation using the dfci teacher model on the dfci test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = pd.read_csv('/mnt/d/Dropbox (Partners HealthCare)/profile_3-2023/derived_data/labeled_medonc_prissmm_mixedisprog.csv')\n",
    "reports = reports[reports.split=='test']\n",
    "inference_input = reports\n",
    "inference_input['text'] = inference_input['text'].str.lower().str.replace(\"\\n\", \" \")\n",
    "inference_input.drop(inference_input.filter(regex='Unnamed|outcome').columns, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import LSTM, Linear, Embedding, Conv1d, MaxPool1d, GRU, LSTMCell, Dropout, Module, Sequential, ReLU\n",
    "\n",
    "   \n",
    "class LabeledModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LabeledModel, self).__init__()\n",
    "        \n",
    "        self.longformer = AutoModel.from_pretrained('yikuan8/Clinical-Longformer')\n",
    "        \n",
    "        self.any_cancer_head = Sequential(Linear(768, 128), ReLU(), Linear(128,1))\n",
    "        self.response_head = Sequential(Linear(768, 128), ReLU(), Linear(128,1))\n",
    "        self.progression_head = Sequential(Linear(768, 128), ReLU(), Linear(128,1))\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x_text_tensor, x_attention_mask):\n",
    "        # x should be tuple of input IDs, then attention mask\n",
    "        global_attention_mask = torch.zeros_like(x_text_tensor).to('cuda')\n",
    "        # global attention on cls token\n",
    "        global_attention_mask[:, 0] = 1\n",
    "        main = self.longformer(x_text_tensor, x_attention_mask, global_attention_mask)\n",
    "        main = main.last_hidden_state[:,0,:].squeeze(1)\n",
    "\n",
    "                                          \n",
    "        any_cancer_out = self.any_cancer_head(main)\n",
    "        response_out = self.response_head(main)\n",
    "        progression_out = self.progression_head(main)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        return any_cancer_out, response_out, progression_out\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class UnLabeledDataset(data.Dataset):\n",
    "    def __init__(self, pandas_dataset):\n",
    "        self.data = pandas_dataset.copy()\n",
    "        self.indices = self.data.index.unique()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"yikuan8/Clinical-Longformer\", truncation_side='left')        \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        # how many notes in the dataset\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # get data for notes corresponding to indices passed\n",
    "        this_index = self.indices[index]\n",
    "        pand = self.data.loc[this_index, :]\n",
    "    \n",
    "        encoded = self.tokenizer(pand['text'], padding='max_length', truncation=True)\n",
    "\n",
    "        x_text_tensor = torch.tensor(encoded.input_ids, dtype=torch.long)\n",
    "        x_attention_mask = torch.tensor(encoded.attention_mask, dtype=torch.long)\n",
    "        \n",
    "\n",
    "        return x_text_tensor, x_attention_mask\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out inference dataset\n",
    "themodel = LabeledModel()\n",
    "themodel.load_state_dict(torch.load('dfci_phi_note_longformer.pt'))\n",
    "themodel.to('cuda')\n",
    "\n",
    "themodel.eval()\n",
    "\n",
    "no_shuffle_valid_dataset = data.DataLoader(UnLabeledDataset(inference_input), batch_size=2, shuffle=False, num_workers=0)\n",
    "\n",
    "output_prediction_lists = [[] for x in range(3)]\n",
    "for batch in no_shuffle_valid_dataset:\n",
    "    x_text_ids = batch[0].to('cuda')\n",
    "    x_attention_mask = batch[1].to('cuda')\n",
    "    with torch.no_grad():\n",
    "        predictions = themodel(x_text_ids, x_attention_mask)\n",
    "    for j in range(3):\n",
    "        output_prediction_lists[j].append(predictions[j].detach().cpu().numpy())\n",
    "\n",
    "output_prediction_lists = [np.concatenate(x) for x in output_prediction_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataset = inference_input.copy()\n",
    "for x in range(3):\n",
    "    output_dataset['outcome_' + str(x) + '_logit'] = output_prediction_lists[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataset = output_dataset.rename(columns={'outcome_0_logit':'any_cancer_logit',\n",
    "                                                  'outcome_1_logit':'response_logit',\n",
    "                                                  'outcome_2_logit':'progression_logit'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_102023 import eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all cancers\n",
      "any_cancer\n",
      "AUC 0.9531537514506728\n",
      "Outcome probability: 0.8024837374334713\n",
      "Average precision score: 0.98\n",
      "Best F1: 0.9624197983501376\n",
      "Best F1 threshold: -0.049199514\n",
      "-0.049199514\n",
      "\n",
      "\n",
      "all cancers\n",
      "progression\n",
      "AUC 0.961429361408972\n",
      "Outcome probability: 0.17060910703725607\n",
      "Average precision score: 0.85\n",
      "Best F1: 0.7908937605396289\n",
      "Best F1 threshold: -0.30774012\n",
      "-0.30774012\n",
      "\n",
      "\n",
      "all cancers\n",
      "response\n",
      "AUC 0.9714942128333686\n",
      "Outcome probability: 0.11975162625665287\n",
      "Average precision score: 0.85\n",
      "Best F1: 0.7995169082125605\n",
      "Best F1 threshold: 0.34003437\n",
      "0.34003437\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for outcome in ['any_cancer','progression','response']:\n",
    "    print('all cancers')\n",
    "    print(outcome)\n",
    "    print(eval_model(output_dataset[outcome + '_logit'], output_dataset[outcome], graph=False))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix nsclc below (has both phase 2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataset['cancer_type'] = np.where(output_dataset.cancer_type.str.contains('nsclc'), 'nsclc', output_dataset.cancer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prostate\n",
      "any_cancer\n",
      "AUC 0.9451417156641037\n",
      "Outcome probability: 0.7528089887640449\n",
      "Average precision score: 0.98\n",
      "Best F1: 0.927070457354759\n",
      "Best F1 threshold: 0.22990912\n",
      "0.22990912\n",
      "\n",
      "\n",
      "prostate\n",
      "progression\n",
      "AUC 0.9551820728291317\n",
      "Outcome probability: 0.09550561797752809\n",
      "Average precision score: 0.73\n",
      "Best F1: 0.7090909090909091\n",
      "Best F1 threshold: -0.5175541\n",
      "-0.5175541\n",
      "\n",
      "\n",
      "prostate\n",
      "response\n",
      "AUC 0.9509147701918785\n",
      "Outcome probability: 0.06741573033707865\n",
      "Average precision score: 0.59\n",
      "Best F1: 0.676056338028169\n",
      "Best F1 threshold: 0.41871846\n",
      "0.41871846\n",
      "\n",
      "\n",
      "breast\n",
      "any_cancer\n",
      "AUC 0.9928493203633318\n",
      "Outcome probability: 0.8935643564356436\n",
      "Average precision score: 1.00\n",
      "Best F1: 0.9862637362637363\n",
      "Best F1 threshold: -3.667033\n",
      "-3.667033\n",
      "\n",
      "\n",
      "breast\n",
      "progression\n",
      "AUC 0.9825996353216984\n",
      "Outcome probability: 0.13613861386138615\n",
      "Average precision score: 0.93\n",
      "Best F1: 0.8571428571428571\n",
      "Best F1 threshold: -1.1806877\n",
      "-1.1806877\n",
      "\n",
      "\n",
      "breast\n",
      "response\n",
      "AUC 0.9709653647752394\n",
      "Outcome probability: 0.14603960396039603\n",
      "Average precision score: 0.89\n",
      "Best F1: 0.8640000000000001\n",
      "Best F1 threshold: -1.4008608\n",
      "-1.4008608\n",
      "\n",
      "\n",
      "nsclc\n",
      "any_cancer\n",
      "AUC 0.9803419169171846\n",
      "Outcome probability: 0.8076572470373746\n",
      "Average precision score: 1.00\n",
      "Best F1: 0.9670206819452207\n",
      "Best F1 threshold: -0.86512107\n",
      "-0.86512107\n",
      "\n",
      "\n",
      "nsclc\n",
      "progression\n",
      "AUC 0.9737383325423192\n",
      "Outcome probability: 0.1959890610756609\n",
      "Average precision score: 0.91\n",
      "Best F1: 0.8495575221238938\n",
      "Best F1 threshold: -0.6937175\n",
      "-0.6937175\n",
      "\n",
      "\n",
      "nsclc\n",
      "response\n",
      "AUC 0.9776193633952255\n",
      "Outcome probability: 0.15405651777575205\n",
      "Average precision score: 0.91\n",
      "Best F1: 0.8597014925373134\n",
      "Best F1 threshold: 1.3636323\n",
      "1.3636323\n",
      "\n",
      "\n",
      "pancreas\n",
      "any_cancer\n",
      "AUC 0.8288477034649476\n",
      "Outcome probability: 0.8232445520581114\n",
      "Average precision score: 0.94\n",
      "Best F1: 0.9603399433427762\n",
      "Best F1 threshold: -0.049199514\n",
      "-0.049199514\n",
      "\n",
      "\n",
      "pancreas\n",
      "progression\n",
      "AUC 0.9677086557309398\n",
      "Outcome probability: 0.13075060532687652\n",
      "Average precision score: 0.79\n",
      "Best F1: 0.784\n",
      "Best F1 threshold: -0.1957723\n",
      "-0.1957723\n",
      "\n",
      "\n",
      "pancreas\n",
      "response\n",
      "AUC 0.978232362968791\n",
      "Outcome probability: 0.09927360774818401\n",
      "Average precision score: 0.79\n",
      "Best F1: 0.8\n",
      "Best F1 threshold: 1.0509207\n",
      "1.0509207\n",
      "\n",
      "\n",
      "rcc_barkouny\n",
      "any_cancer\n",
      "AUC 0.7914049586776859\n",
      "Outcome probability: 0.9063670411985019\n",
      "Average precision score: 0.96\n",
      "Best F1: 0.983739837398374\n",
      "Best F1 threshold: 0.9678137\n",
      "0.9678137\n",
      "\n",
      "\n",
      "rcc_barkouny\n",
      "progression\n",
      "AUC 0.8897761194029851\n",
      "Outcome probability: 0.250936329588015\n",
      "Average precision score: 0.69\n",
      "Best F1: 0.7297297297297297\n",
      "Best F1 threshold: 3.4486113\n",
      "3.4486113\n",
      "\n",
      "\n",
      "rcc_barkouny\n",
      "response\n",
      "AUC 0.9491690049643858\n",
      "Outcome probability: 0.15355805243445692\n",
      "Average precision score: 0.82\n",
      "Best F1: 0.7741935483870966\n",
      "Best F1 threshold: -0.5209047\n",
      "-0.5209047\n",
      "\n",
      "\n",
      "crc\n",
      "any_cancer\n",
      "AUC 0.9920109045473477\n",
      "Outcome probability: 0.6901408450704225\n",
      "Average precision score: 1.00\n",
      "Best F1: 0.9770114942528736\n",
      "Best F1 threshold: -3.2873983\n",
      "-3.2873983\n",
      "\n",
      "\n",
      "crc\n",
      "progression\n",
      "AUC 0.9580739457586748\n",
      "Outcome probability: 0.18309859154929578\n",
      "Average precision score: 0.86\n",
      "Best F1: 0.8\n",
      "Best F1 threshold: -0.30774012\n",
      "-0.30774012\n",
      "\n",
      "\n",
      "crc\n",
      "response\n",
      "AUC 0.9781946559601273\n",
      "Outcome probability: 0.06237424547283702\n",
      "Average precision score: 0.75\n",
      "Best F1: 0.7164179104477612\n",
      "Best F1 threshold: 1.1187909\n",
      "1.1187909\n",
      "\n",
      "\n",
      "bladder\n",
      "any_cancer\n",
      "AUC 0.9666666666666667\n",
      "Outcome probability: 0.8235294117647058\n",
      "Average precision score: 0.99\n",
      "Best F1: 0.967741935483871\n",
      "Best F1 threshold: 1.3013607\n",
      "1.3013607\n",
      "\n",
      "\n",
      "bladder\n",
      "progression\n",
      "AUC 0.9204545454545454\n",
      "Outcome probability: 0.25882352941176473\n",
      "Average precision score: 0.82\n",
      "Best F1: 0.7659574468085107\n",
      "Best F1 threshold: -0.18355222\n",
      "-0.18355222\n",
      "\n",
      "\n",
      "bladder\n",
      "response\n",
      "AUC 0.9756036217303823\n",
      "Outcome probability: 0.16470588235294117\n",
      "Average precision score: 0.90\n",
      "Best F1: 0.8571428571428571\n",
      "Best F1 threshold: 1.7513282\n",
      "1.7513282\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/Dropbox (Partners HealthCare)/impression_bert/prissmm_medonc/utils_102023.py:111: RuntimeWarning: invalid value encountered in divide\n",
      "  F1 = 2*((precision*recall)/(precision+recall))\n"
     ]
    }
   ],
   "source": [
    "for cancer in output_dataset.cancer_type.unique():\n",
    "    subset = output_dataset[output_dataset.cancer_type == cancer]\n",
    "    for outcome in ['any_cancer','progression','response']:\n",
    "        print(cancer)\n",
    "        print(outcome)\n",
    "        print(eval_model(subset[outcome + '_logit'], subset[outcome], graph=False))\n",
    "        print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
